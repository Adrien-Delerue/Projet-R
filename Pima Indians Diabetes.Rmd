---
title: "PROJECT 4"
output: 
  html_document:
    toc: true     
    toc_float: true
    df_print: paged
    css: "Pima_Indians_Diabetes.css"
date: "2025-10-22"
---

# Pima Diabetes -Detecting Diabetes from Imperfect Medical Data

The project aims to predict diabetes in Pima Indian women using imperfect medical data.

After cleaning invalid values, features are standardized and categorized, visualizations are created to explore patterns, and models (Logistic Regression and KNN) are trained and compared to show the impact of data cleaning.

```{r include=FALSE}
#install.packages("dplyr")
library(dplyr)
library(gridExtra)

```

```{r echo=FALSE}
# Loading the dataset
raw_diabetes = read.csv("diabetes.csv", head=TRUE, sep=",")
summary(raw_diabetes)
```

# Step 1 : Data Cleaning and Preparation

First we are going to replace all the impossible values, such as :

-   the Glucose level at 0

-   the Blood Pressure at 0

-   the Skin Thickness at 0

-   the BMI at 0

-   and the Diabetes Pedigree Function at 0

We obtain the following summary :

```{r echo=FALSE}
# Mutating each columns concern, if a value equals 0, it's replaced by an NA value
selected_col <- c("Glucose", "BloodPressure", "SkinThickness", "BMI", "DiabetesPedigreeFunction")
diabetes <- raw_diabetes %>% mutate(across(all_of(selected_col), ~ na_if(., 0))) # Replace all 0 values with NA in the selected columns
# Across : apply the function for each col
# ~ na_if(., 0) : replace 0 by NA for each col selected
# . : current col
# all_of : column names taken from a variable.
summary(diabetes)
```

Now we are going to replace those NA values by the median of each columns. We obtain the following summary :

```{r echo=FALSE}
# Mutating each columns concern, if a value equals NA, it's replaced by the median of the corresponding columns else it stays the same
diabetes <- diabetes %>% mutate(across(all_of(selected_col), ~ if_else(is.na(.), median(., na.rm = TRUE), .)))
summary(diabetes)
```

This is the **post cleaning verification**, unlike the previous summary we can see in this one that we don't have any NA Values left. Our data set is now cleared.

# Step 2 : Feature Transformation and Engineering

First, we create 2 new variables :

-   BMICategory in function of the BMI, which is based on the standard World Health Organisation(WHO) thresholds, into 4 categories( Underweight, Normal, Overweight, Obesity)

-   BPCategory in function of the Blood Pressure, according to the World Health Organisation thresholds, into 5 categories( Normal, Elevated, Hypertension Level 1 and 2 and Hypertension crisis)

Then we transform the Outcome variable into text to make it more understandable. If the value is 1, then we write "Diabetic", otherwise we write "Not Diabetic".

```{r}
# Mutating each column needed to be categorized
diabetes <- diabetes %>% mutate(
  BMICategory = case_when(
    BMI < 18.5 ~ "Underweight",
    BMI < 24.9 ~ "Normal", # Equals to BMI >= 18.5 and BMI < 24.9 because the first case has been complete
    BMI < 29.9 ~ "Overweight",
    .default = "Obesity"
  ),
  BPCategory = case_when(
     BloodPressure < 80 ~ "Normal",
     BloodPressure < 90 ~ "Elevated",
     BloodPressure < 100 ~ "Hypertension Level 1",
     BloodPressure < 110 ~ "Hypertension Level 2",
     .default = "Hypertension crisis"
  ),
  Outcome = case_when(
    Outcome == 1 ~ "Diabetic",
    .default = "Not Diabetic"
  )
)
diabetes
```

Then we want to standardize our numerical values, so we subtract by the mean and divide by the standard deviation to have a mean of 0 and a standard deviation of 1. This will be better to train the models later.

```{r echo=FALSE}
# Standardizing all numeric columns (mean=0, sd=1)
diabetes_scaled <- diabetes %>% mutate(across(where(is.numeric), scale))
diabetes_scaled
```

# Step 3: Visualization and Exploration (with ggplot2)

`cleaned_outcome_diabetes` is our data set before the first step, so not cleaned at all. with missing values and incoherences.

```{r}
cleaned_outcome_diabetes <- raw_diabetes %>% mutate(
  Outcome = case_when(
    Outcome == 1 ~ "Diabetic",
    .default = "Not Diabetic"
  )
)
library(ggplot2)
p1 <- ggplot(cleaned_outcome_diabetes, aes(x = Glucose, fill = as.factor(Outcome))) +
  geom_density(alpha = 0.5) +
  labs(title = "Glucose (not cleaned)", x = "Glucose", y = "Density") +
  theme_minimal()

p2 <- ggplot(diabetes, aes(x = Glucose, fill = as.factor(Outcome))) +
  geom_density(alpha = 0.5) +
  labs(title = "Glucose (cleaned)", x = "Glucose", y = "Density") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

For the two graphs:

The **red curve** represent non-diabetic's patient. Their curve is centered around lower glucose values. In the other hand, the **blue curve** are diabetic's patient is shifted to the right, indicating higher glucose levels. The blue curve extends further right, suggesting greater variability in glucose levels among diabetics.

For the second graph, as we cleaned data (where glucose's value was 0) the x scale is shifted to the right. We can see that the overlap is reduced, making it easier to distinguish between the two groups. (That's why its really important to clean our data before analyse them, in order to make a better investigation)

These curves show that the glucose variable is a good indicator for distinguishing diabetic patients from non-diabetic patients. It could be a key variable in a predictive model.

Now let's compare distribution for the 'BMI' and 'Age' variables to show their distribution between diabetic and non-diabetic patients.

```{r}
p3 <- ggplot(diabetes, aes(x = as.factor(Outcome), y = BMI, fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "BMI by Outcome", x = "Outcome", y = "BMI") +
  theme_minimal()

p4 <- ggplot(diabetes, aes(x = as.factor(Outcome), y = Age, fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "Age by Outcome", x = "Outcome", y = "Âge") +
  theme_minimal()

grid.arrange(p3, p4, ncol = 2)
```

For the BMI graph, diabetic patients often have a higher BMI (overweight/obese), which is a known risk factor for diabetes. That's why the box plots show a higher median for diabetics.

And for the age graph, type 2 diabetes is more common in older people, so the age distribution shows a higher concentration of diabetics in the older age groups.

Now we can compared theses graphs with those we would have had if we hadn't cleaned our data

```{r}
p5 <- ggplot(cleaned_outcome_diabetes, aes(x = as.factor(Outcome), y = BMI, 
                                        fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "BMI (not cleaned)", x = "Outcome", y = "BMI") +
  theme_minimal()

p6 <- ggplot(cleaned_outcome_diabetes, aes(x = as.factor(Outcome), y = Age, 
                           fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "BMI (cleaned)", x = "Outcome", y = "BMI") +
  theme_minimal()

grid.arrange(p5, p6, ncol = 2)
```

The age graph is no different from that of the cleaned dataset. This is simply because we did not modify the “age” column during the data cleaning step.

Unlike the BMI graph of the uncleaned data, where we see values such as 0 appearing, which can alter the median value.

```{r}
# Select only numerical variables (exclude Outcome if it's a factor)
numerical_vars <- diabetes %>% 
  select(where(is.numeric), -Outcome)  # Exclude Outcome from correlation

correlation_matrix <- cor(numerical_vars, use = "complete.obs")

# Print correlation matrix
print(round(correlation_matrix, 2))

library(reshape2)  

correlation_long <- melt(correlation_matrix)

# Create heatmap
ggplot(correlation_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1),
                       name = "Correlation") +
  labs(title = "Correlation Heatmap of Medical Variables",
       x = "", y = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        plot.title = element_text(hjust = 0.5, face = "bold"))
```

We can see that **age and pregnancy** are fairly correlated, which makes sense. Generally, people are pregnant between the ages of 20 and 45, rarely younger and rarely older. In a same way with **BMI & SkinThickness** variable: These measurements are physiologically related

The absence of severe multicollinearity indicates that all variables can be safely included in our predictive models without redundancy issues.

# Step 4 : Modeling and Predicting (Data Science)

First we are going to divide our cleaned data set in 2 groups, the training dataset (80% of our data) and the testing dataset (20% of our data)

```{r include=FALSE}
# Randomly shuffling the data
set.seed(123)

#determining the size of the sampling
sampleIndex <- sample(1:nrow(diabetes_scaled), 0.8 * nrow(diabetes_scaled))

#splitting our cleaned and scaled data
trainData <- diabetes_scaled[sampleIndex, ]
testData <- diabetes_scaled[-sampleIndex, ]
```

We are going to use 4 different algorithms, logistic Regression Model, K-nearest Model and Cross-Validation, then we'll analyze the impact of cleaning

But before we apply any of our model we are going to prepare our data, separating the input from the outputs

```{r include=FALSE}
#we take out the non-numerical and the Outcome out of our input data
trainFeatures <- trainData[, 1:(ncol(trainData)-3)]
testFeatures  <- testData[, 1:(ncol(testData)-3)]

#we prepare our training output
trainLabels   <- trainData$Outcome
```

### Logistic Regression Model

```{r echo=FALSE}
#adapting the data to the model
trainDF <- trainData  %>% select(-BMICategory, -BPCategory)
trainDF$Outcome <- ifelse(trainDF$Outcome == "Diabetic", 1, 0)
```

```{r echo=FALSE}
# training the model
logistic_model <- glm(Outcome ~ ., data = trainDF, family = binomial)

# Prediction on our test data
logistic_prediction <- ifelse(predict(logistic_model, newdata = testFeatures, type ="response") > 0.5,  1, 0)

table(logistic_prediction, testData$Outcome)
```

the line on the table are the is the reality and the columns if the predicted. Our model predicted 119 right out of 154, it also predicted 12 false positive and 23 false negative

### K-Nearest Neighbors Model

```{r include=FALSE}
#downloading the libraries
library(class)
```

We are going to apply our model K Nearest Neighbors with 5 Neighbors

```{r echo=FALSE}
#applying the KNN algorithm
knn_predictions <- knn(train = trainFeatures, test = testFeatures, cl = trainLabels, k=5)

table(knn_predictions, testData$Outcome)

```

the line on the table are the is the reality and the columns if the predicted. Our model predicted 121 right out of 154, it also predicted 11 false positive and 22 false negative

### Cross Validation

We trained and tested our data set with a 80% of the data to train each of our model and 20% to test it Now we are going to observe the results for each of our models with a different split to evaluate their robustness and define which one is better

For that We are going to divide our data set in five equal part and train our model 15 time. On each training we'll train the model on 14 folds then test it on the 15th

```{r include=FALSE}
#Preparing the data to be treated by the algorithms
trainCV <- diabetes_scaled  %>% select(-BMICategory, -BPCategory)
trainCV$Outcome <- ifelse(trainCV$Outcome == "Diabetic", 1, 0)

#determining the number of folds
n<-15

#slitting the data into n folds
folds <- split(trainCV, factor(sort(rank(row.names(trainCV))%%n)))
```

```{r echo = FALSE}
#creating the variable with which we'll record the 
accuracyLR<-c()
accuracyKNN <-c()

for(i in 1:n){
  test_fold  <- folds[[i]]
  train_fold <- do.call(rbind, folds[-i])
  
  #Logistic Regression Model 
  logistic_model <- glm(Outcome ~ ., data = train_fold, family = binomial)
  # Prediction on our test data
  logistic_prediction <- ifelse(predict(logistic_model, newdata = test_fold%>%select(-Outcome), type ="response") > 0.5,  1, 0)
  
  #Registering the accuracy for this fold
  accuracyLR <- c(accuracyLR, mean(logistic_prediction == test_fold$Outcome))
  
  
  #KNN Model 
  knn_predictions <- knn(train = train_fold%>%select(-Outcome), test = test_fold%>%select(-Outcome), cl = train_fold$Outcome, k=5)
  
  #Registering the accuracy for this fold
  accuracyKNN <- c(accuracyKNN, mean(knn_predictions == test_fold$Outcome))
}

cat(sprintf("Mean accuracy for Logistic Regression Model : %.3f\n", mean(accuracyLR)))
cat(sprintf("Mean accuracy for K-Nearest Neighbors Model : %.3f\n", mean(accuracyKNN)))
```

After doing our Cross-Validation we can observe that our accuracy is higher when using the Logistic Regression Model than the K-Nearest Neighbors Model with accuracies of 77.2% and 73% respectively

To check the robustness of the model we are going to plot the accuracies for all our 15 iterations

```{r echo=FALSE}
df <- data.frame(
  Fold = 1:n,
  LRModel = accuracyLR,
  KNNModel = accuracyKNN
)

ggplot(df, aes(x=Fold)) +
  geom_line(aes(y=LRModel, color="Logistic Regression")) +
  geom_line(aes(y=KNNModel, color="K-Nearest Neighbors")) +
  ylim(0,1) +
  labs(title="Logistic Regression vs K-Nearest Neighbors Model Accuracy", y="Accuracy") +
  scale_color_manual("", values=c("K-Nearest Neighbors"="blue", "Logistic Regression"="red"))
```

We can observe on the plot that the accuracy of the Logistic Regression Model is much more consistent. We can determine that the Logistic Regression Model is the more robust of the two and will give better results

### Analyzing the Impact of Cleaning

First we'll scaled our uncleaned data

```{r include=FALSE}
#Preparing the data to be treated by the algorithms
trainDirty <- not_cleaned_diabetes%>% mutate(across(where(is.numeric)& !last_col(), scale))

#determining the number of folds

#slitting the data into n folds
foldsDirty <- split(trainDirty, factor(sort(rank(row.names(trainDirty))%%n)))
```

Then we'll Cross-validate it like we did previously

```{r echo = FALSE}
#creating the variable with which we'll record the accuracy of the model 
accuracyLRDirty<-c()

for(i in 1:n){
  test_fold  <- foldsDirty[[i]]
  train_fold <- do.call(rbind, foldsDirty[-i])
  
  #Logistic Regression Model 
  logistic_model <- glm(Outcome ~ ., data = train_fold, family = binomial)
  # Prediction on our test data
  logistic_prediction <- ifelse(predict(logistic_model, newdata = test_fold%>%select(-Outcome), type ="response") > 0.5,  1, 0)
  
  #Registering the accuracy for this fold
  accuracyLRDirty <- c(accuracyLRDirty, mean(logistic_prediction == test_fold$Outcome))
}

cat(sprintf("Mean accuracy for Logistic Regression Model before cleaning : %.3f\n", mean(accuracyLRDirty)))
```

We obtain an accuracy of 77.6% which slightly higher than the accuracy of the same model with cleaned data (77.2%) This could come from the higher variance in the unclean data But That doesn't indicate that the using unclean data make the model more robust. To observe that we are going to plot the accuracies on the folds

```{r echo=FALSE}
df <- data.frame(
  Fold = 1:n,
  dirty_data = accuracyLRDirty,
  cleaned_data = accuracyLR
)

ggplot(df, aes(x=Fold)) +
  geom_line(aes(y=dirty_data, color="Dirty Data")) +
  geom_line(aes(y=cleaned_data, color="Cleaned Data")) +
  ylim(0,1) +
  labs(title="Dirty Data vs Cleaned Data Accuracy", y="Accuracy") +
  scale_color_manual("", values=c("Dirty Data"="blue", "Cleaned Data"="red"))
```

We can observe that there the Model's accuracy is more stable when using the cleaned data and therefore more robust. Using cleaned Data is of critical important to avoid overfitting and have a better generalisation of our model
