---
title: "PROJECT 4"
output: 
  html_document:
    toc: true     
    toc_float: true
    df_print: paged
    css: "Pima_Indians_Diabetes.css"
date: "2025-10-22"
---

# Pima Diabetes -Detecting Diabetes from Imperfect Medical Data

The project aims to predict diabetes in Pima Indian women using imperfect medical data.

After cleaning invalid values, features are standardized and categorized, visualizations are created to explore patterns, and models (Logistic Regression and KNN) are trained and compared to show the impact of data cleaning.

```{r include=FALSE}
library(dplyr)
```

```{r echo=FALSE}
# Loading the dataset
diabetes = read.csv("diabetes.csv", head=TRUE, sep=",")
summary(diabetes)
```

# Step 1 : Data Cleaning and Preparation

First we are going to replace all the impossible values, such as :

-   the Glucose level at 0

-   the Blood Pressure at 0

-   the Skin Thickness at 0

-   the BMI at 0

-   and the Diabetes Pedigree Function at 0

We obtain the following summary :

```{r echo=FALSE}
# Mutating each columns concern, if a value equals 0, it's replaced by an NA value
selected_col <- c("Glucose", "BloodPressure", "SkinThickness", "BMI", "DiabetesPedigreeFunction")
diabetes <- diabetes %>% mutate(across(all_of(selected_col), ~ na_if(., 0))) # Replace all 0 values with NA in the selected columns
# Across : apply the function for each col
# ~ na_if(., 0) : replace 0 by NA for each col selected
# . : current col
# all_of : column names taken from a variable.
summary(diabetes)
```

Now we are going to replace those NA values by the median of each columns. We obtain the following summary :

```{r echo=FALSE}
# Mutating each columns concern, if a value equals NA, it's replaced by the median of the corresponding columns else it stays the same
diabetes <- diabetes %>% mutate(across(all_of(selected_col), ~ if_else(is.na(.), median(., na.rm = TRUE), .)))
summary(diabetes)
```

This is the **post cleaning verification**, unlike the previous summary we can see in this one that we don't have any NA Values left. Our data set is now cleared.

# Step 2 : Feature Transformation and Engineering

First, we create 2 new variables :

-   BMICategory in function of the BMI, which is based on the standard World Health Organisation(WHO) thresholds, into 4 categories( Underweight, Normal, Overweight, Obesity)

-   BPCategory in function of the Blood Pressure, according to the World Health Organisation thresholds, into 5 categories( Normal, Elevated, Hypertension Level 1 and 2 and Hypertension crisis)

Then we transform the Outcome variable into text to make it more understandable. If the value is 1, then we write "Diabetic", otherwise we write "Not Diabetic".

```{r}
# Mutating each column needed to be categorized
diabetes <- diabetes %>% mutate(
  BMICategory = case_when(
    BMI < 18.5 ~ "Underweight",
    BMI < 24.9 ~ "Normal", # Equals to BMI >= 18.5 and BMI < 24.9 because the first case has been complete
    BMI < 29.9 ~ "Overweight",
    .default = "Obesity"
  ),
  BPCategory = case_when(
     BloodPressure < 80 ~ "Normal",
     BloodPressure < 90 ~ "Elevated",
     BloodPressure < 100 ~ "Hypertension Level 1",
     BloodPressure < 110 ~ "Hypertension Level 2",
     .default = "Hypertension crisis"
  ),
  Outcome = case_when(
    Outcome == 1 ~ "Diabetic",
    .default = "Not Diabetic"
  )
)
diabetes
```

Then we want to standardize our numerical values, so we subtract by the mean and divide by the standard deviation to have a mean of 0 and a standard deviation of 1. This will be better to train the models later.

```{r echo=FALSE}
# Standardizing all numeric columns (mean=0, sd=1)
diabetes_scaled <- diabetes %>% mutate(across(where(is.numeric), scale))
diabetes_scaled
```

# Step 3: Visualization and Exploration (with ggplot2)

```{r}
not_cleaned_diabetes = read.csv("diabetes.csv", head=TRUE, sep=",")
```

`not_cleaned_diabete` is our data set before the first step, so not cleaned at all. with missing values and incoherences.

```{r}
library(ggplot2)
p1 <- ggplot(not_cleaned_diabetes, aes(x = Glucose, fill = as.factor(Outcome))) +
  geom_density(alpha = 0.5) +
  labs(title = "Glucose (not cleaned)", x = "Glucose", y = "Density") +
  theme_minimal()

p2 <- ggplot(diabetes, aes(x = Glucose, fill = as.factor(Outcome))) +
  geom_density(alpha = 0.5) +
  labs(title = "Glucose (cleaned)", x = "Glucose", y = "Density") +
  theme_minimal()

library(gridExtra)
grid.arrange(p1, p2, ncol = 2)
```

For the two graphs:

The **red curve** represent non-diabetic's patient. Their curve is centered around lower glucose values. In the other hand, the **blue curve** are diabetic's patient is shifted to the right, indicating higher glucose levels. The blue curve extends further right, suggesting greater variability in glucose levels among diabetics.

For the second graph, as we cleaned data (where glucose's value was 0) the x scale is shifted to the right. We can see that the overlap is reduced, making it easier to distinguish between the two groups. (That's why its really important to clean our data before analyse them, in order to make a better investigation)

These curves show that the glucose variable is a good indicator for distinguishing diabetic patients from non-diabetic patients. It could be a key variable in a predictive model.

```{r}
p3 <- ggplot(diabetes, aes(x = as.factor(Outcome), y = BMI, fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "BMI by Outcome", x = "Outcome", y = "BMI") +
  theme_minimal()

p4 <- ggplot(diabetes, aes(x = as.factor(Outcome), y = Age, fill = as.factor(Outcome))) +
  geom_boxplot() +
  labs(title = "Âge by Outcome", x = "Outcome", y = "Âge") +
  theme_minimal()

grid.arrange(p3, p4, ncol = 2)
```

For the BMI graph, diabetic patients often have a higher BMI (overweight/obese), which is a known risk factor for diabetes. That's why the box plots show a higher median for diabetics.

And for the age graph, type 2 diabetes is more common in older people, so the age distribution shows a higher concentration of diabetics in the older age groups.

```{r}
#install.packages("ggcorrplot") 
library(ggcorrplot)

numerical_data <- diabetes %>% select(where(is.numeric))
cor_matrix <- cor(numerical_data, use = "complete.obs")

biserial_cor <- sapply(numerical_data, function(x) cor(x, diabetes$Outcome, method = "pearson"))

cor_matrix_with_outcome <- rbind(cor_matrix, biserial_cor)
rownames(cor_matrix_with_outcome)[nrow(cor_matrix_with_outcome)] <- "Outcome"
colnames(cor_matrix_with_outcome)[ncol(cor_matrix_with_outcome)] <- "Outcome"


ggcorrplot(
  cor_matrix_with_outcome,
  hc.order = TRUE,
  type = "upper",
  lab = TRUE,
  lab_size = 3,
  method = "circle",
  colors = c("tomato2", "white", "springgreen3"),
  title = "Correlation Matrix (Including Outcome)",
  show.legend = TRUE,
  legend.title = "Correlation"
)
```

We can see that age and pregnancy are fairly correlated, which makes sense. Generally, people are pregnant between the ages of 20 and 45, rarely younger and rarely older.

# Step 4 : Modeling and Predicting (Data Science)

First we are going to divide our cleaned data set in 2 groups, the training dataset (80% of our data) and the testing dataset (20% of our data)

```{r include=FALSE}
# Randomly shuffling the data
set.seed(123)

#determining the size of the sampling
sampleIndex <- sample(1:nrow(diabetes_scaled), 0.8 * nrow(diabetes_scaled))

#splitting our cleaned and scaled data
trainData <- diabetes_scaled[sampleIndex, ]
testData <- diabetes_scaled[-sampleIndex, ]
```

We are going to use 4 different algorithms, logistic Regression Model, K-nearest Model and Cross-Validation, then we'll analyze the impact of cleaning

### Logistic Regression Model

### K-Nearest Model

```{r include=FALSE}
#downloading the libraries
library(class)
```

First we prepare our data, separating the input from the outputs

```{r include=FALSE}
#we take out the non-numerical and the Outcome out of our input data
trainFeatures <- trainData[, 1:(ncol(trainData)-3)]
testFeatures  <- testData[, 1:(ncol(testData)-3)]

#we prepare our training output
trainLabels   <- trainData$Outcome
```

then we apply our model

```{r echo=FALSE}
#applying the KNN algorithm
knn_predictions <- knn(train = trainFeatures, test = testFeattures, cl = trainLabels, k=5)

table(knn_predictions, testData$Outcome)
```

the line on the table are the is the reality and the columns if the predicted. Our model predicted 121 right out of 154, it also predicted 11 false positive and 22 false negative

### Cross Validation

